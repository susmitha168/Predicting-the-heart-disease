{"cells":[{"metadata":{},"cell_type":"markdown","source":"Data Set Information:\n\nThis database contains 76 attributes, but all published experiments refer to using a subset of 14 of them. In particular, the Cleveland database is the only one that has been used by ML researchers to this date. The \"goal\" field refers to the presence of heart disease in the patient. It is integer valued from 0 (no presence) to 4. Experiments with the Cleveland database have concentrated on simply attempting to distinguish presence (values 1,2,3,4) from absence (value 0).\n\nThe names and social security numbers of the patients were recently removed from the database, replaced with dummy values.\n\nOne file has been \"processed\", that one containing the Cleveland database. All four unprocessed files also exist in this directory.\n\nTo see Test Costs (donated by Peter Turney), please see the folder \"Costs\"\n\nAttribute Information:\n\nOnly 14 attributes used: 1. #3 (age) 2. #4 (sex) 3. #9 (cp) 4. #10 (trestbps) 5. #12 (chol) 6. #16 (fbs) 7. #19 (restecg) 8. #32 (thalach) 9. #38 (exang) 10. #40 (oldpeak) 11. #41 (slope) 12. #44 (ca) 13. #51 (thal) 14. #58 (num) (the predicted attribute)\n\nColumns:\n    age:age in years\n    sex:(1 = male; 0 = female)\n    cp:chest pain type\n    trestbps:resting blood pressure (in mm Hg on admission to the hospital)\n    chol:serum cholestoral in mg/dl\n    fbs:(fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\n    restecg:resting electrocardiographic results\n    thalach:maximum heart rate achieved\n    exang:exercise induced angina (1 = yes; 0 = no)\n    oldpeak:ST depression induced by exercise relative to rest\n    slope:the slope of the peak exercise ST segment\n    ca:number of major vessels (0-3) colored by flourosopy\n    thal : 3 = normal; 6 = fixed defect; 7 = reversable defect\n    target:1 or 0 \n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np\nimport pandas as pd\nimport pandas_profiling as pp\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":35,"outputs":[{"output_type":"stream","text":"['heart-disease-uci']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Loading the data into dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/heart-disease-uci/heart.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Viewing the data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Once the data is loaded,we can view the data. Instead of viewing the entire data , we can view the first five rows\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can view the last five rows of the data\ndf.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Viewing the size of the data (no of rows and no of columns)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#we can view the total number of rows and columns of data\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Understanding the column attributes - datatype , no of non null rows"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we got the information about the total size of the data . We can further explore the detailed information about each column using the info method on datadrame\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The ProfileReport() method gives detailed data summary statistics "},{"metadata":{"trusted":true},"cell_type":"code","source":"profile = pp.ProfileReport(df)\nprofile","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can get the summary statistics (min,max,count...) of each column by using the describe() method\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking the column names\ndf.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rename the columns of dataframe to more meaningful\ndf=df.rename(columns={'age':'Age','sex':'Sex','cp':'Cp','trestbps':'Trestbps','chol':'Chol','fbs':'Fbs','restecg':'Restecg','thalach':'Thalach','exang':'Exang','oldpeak':'Oldpeak','slope':'Slope','ca':'Ca','thal':'Thal','target':'Target'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Recheck the column names\ndf.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for missing values\n#df.isnull().sum()\ndf.isnull().mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Detecting the Outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the IQR for entire dataset, to detect outliers\nQ1 = df.quantile(0.25)\nQ3 = df.quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filtering the columns by removing the outliers\nprint((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))  )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Try to delete the rows with outliers and check if this impacts our prediction\ndf_out = df[(df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))] #viewing the outliers\nprint(df_out)\n#df_out = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df_out.shape # We can see that by deleting the rows with outliers , we may lose a large amount of data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will now try perform imputation on these outliers , As all columns are numerical we can perform median imputation\n#df.out = df[(df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Identifying the duplicated values"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will try to check and drop duplicate values\n#Incase of duplicate rows we use drop_duplicates() method\ndf_dup = df[df.duplicated()]\ndf_dup","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#As we have one duplicate row , Delete the duplicated rows\ndf = df.drop_duplicates()\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df['Target'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Converting the categorical variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.get_dummies(df, columns = ['Sex', 'Cp', 'Fbs', 'Restecg', 'Exang', 'Slope', 'Ca', 'Thal'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Performing Feature Scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"standardScaler = StandardScaler()\ncolumns_to_scale = ['Age', 'Trestbps', 'Chol', 'Thalach', 'Oldpeak']\ndf[columns_to_scale] = standardScaler.fit_transform(df[columns_to_scale])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df['Target']\nX = df.drop(['Target'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg = LogisticRegression()\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.4,random_state = 42)\nlogreg.fit(X_train,y_train)\ny_pred = logreg.predict(X_test)\nprint(\"The Score is\",logreg.score(X_test,y_test))\ncv_score_five = cross_val_score(logreg,X,y,cv=5)\ncv_score_ten = cross_val_score(logreg,X,y,cv=10)\nprint(\"Cross validation score - Five Folds\",cv_score_five)\nprint(\"Cross validation score - Ten Folds\",cv_score_ten)\nprint(\"Mean cross validation score\",cv_score_ten.mean())\nprint(\"Confusion Matrix\",confusion_matrix(y_test,y_pred))\nprint(\"Classification Report\")\nprint(classification_report(y_test,y_pred))","execution_count":38,"outputs":[{"output_type":"stream","text":"The Score is 0.8429752066115702\nCross validation score - Five Folds [0.81967213 0.8852459  0.85245902 0.85       0.74576271]\nCross validation score - Ten Folds [0.87096774 0.77419355 0.87096774 0.87096774 0.9        0.76666667\n 0.86666667 0.9        0.68965517 0.75862069]\nMean cross validation score 0.8268705969595848\nConfusion Matrix [[43  9]\n [10 59]]\nClassification Report\n              precision    recall  f1-score   support\n\n           0       0.81      0.83      0.82        52\n           1       0.87      0.86      0.86        69\n\n    accuracy                           0.84       121\n   macro avg       0.84      0.84      0.84       121\nweighted avg       0.84      0.84      0.84       121\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Decision Tree Classfier"},{"metadata":{"trusted":true},"cell_type":"code","source":"dec_clf = DecisionTreeClassifier(max_depth = 3 , random_state=1)\ndec_clf.fit(X_train,y_train)\ny_pred = dec_clf.predict(X_test)\nprint(\"The accuarcy score of decision tree classifier is \",accuracy_score(y_test,y_pred))\ncv_dec_tree_clf = cross_val_score(dec_clf,X,y,cv=10)\nprint(\"The Cross validation score \",cv_dec_tree_clf.mean())","execution_count":36,"outputs":[{"output_type":"stream","text":"The accuarcy score of decision tree classifier is  0.8429752066115702\nThe Cross validation score  0.8069929551353356\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"randomforest_classifier= RandomForestClassifier(n_estimators=10)\nscore=cross_val_score(randomforest_classifier,X,y,cv=10)\nprint(score.mean())","execution_count":37,"outputs":[{"output_type":"stream","text":"0.8136744530960327\n","name":"stdout"}]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}